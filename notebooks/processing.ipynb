{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyreadstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2779017/3810890782.py:1: DtypeWarning: Columns (11,12,13,14,68,296,318,335,339,341,349,351,372,374,387) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  R9 = pd.read_csv('/work/lamlab/data/R9.csv')\n",
      "/tmp/ipykernel_2779017/3810890782.py:2: DtypeWarning: Columns (43,45,47,49,53,169,191,387) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  R8 = pd.read_csv('/work/lamlab/data/R8.csv')\n",
      "/tmp/ipykernel_2779017/3810890782.py:3: DtypeWarning: Columns (9,10,11,12,13,15,253,282,311,313,334,336,349) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  R7 = pd.read_csv('/work/lamlab/data/R7.csv')\n"
     ]
    }
   ],
   "source": [
    "R9 = pd.read_csv('/work/lamlab/data/R9.csv')\n",
    "R8 = pd.read_csv('/work/lamlab/data/R8.csv')\n",
    "R7 = pd.read_csv('/work/lamlab/data/R7.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "R9['ROUND'] = 9\n",
    "R8['ROUND'] = 8\n",
    "R7['ROUND'] = 7\n",
    "\n",
    "columns = [\n",
    "    'COUNTRY',\n",
    "    'EA_GPS_LA',\n",
    "    'EA_GPS_LO',\n",
    "    'EA_SVC_A',\n",
    "    'EA_SVC_B',\n",
    "    'EA_SVC_C',\n",
    "    'URBRUR',\n",
    "    'REGION',\n",
    "    'ROUND'\n",
    "    ]\n",
    "\n",
    "R9 = R9[columns]\n",
    "R8 = R8[columns]\n",
    "R7 = R7[columns]\n",
    "\n",
    "R9 = R9.groupby(['EA_GPS_LA', 'EA_GPS_LO'], as_index=False).mean()\n",
    "R8 = R8.groupby(['EA_GPS_LA', 'EA_GPS_LO'], as_index=False).mean()\n",
    "R7 = R7.groupby(['EA_GPS_LA', 'EA_GPS_LO'], as_index=False).mean()\n",
    "\n",
    "R9= R9[R9['EA_SVC_C'].isin([0, 1])]\n",
    "R8= R8[R8['EA_SVC_C'].isin([0, 1])]\n",
    "R7= R7[R7['EA_SVC_C'].isin([0, 1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([R7, R8, R9], ignore_index=True)\n",
    "combined_df.to_csv('/work/lamlab/MERGED.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure for pretraining on R7, R8, and R9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading CSV file: [Errno 2] No such file or directory: '/work/lamlab/data/MERGED.csv'\n",
      "Scanning .tif files and matching coordinates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files in R7: 0file [00:00, ?file/s]\n",
      "Processing files in R7: 0file [00:00, ?file/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files in R7: 100%|██████████| 5038/5038 [00:02<00:00, 2007.07file/s]\n",
      "Processing files in R7: 100%|██████████| 3618/3618 [00:02<00:00, 1440.36file/s]\n",
      "Processing files in R7: 100%|██████████| 5687/5687 [00:03<00:00, 1551.92file/s]\n",
      "Processing files in R7: 100%|██████████| 3956/3956 [00:02<00:00, 1612.14file/s]\n",
      "Processing files in R7: 100%|██████████| 5432/5432 [00:03<00:00, 1367.97file/s]\n",
      "Processing files in R7: 100%|██████████| 3680/3680 [00:02<00:00, 1529.94file/s]\n",
      "Processing files in R7: 100%|██████████| 4110/4110 [00:03<00:00, 1275.96file/s]\n",
      "Processing files in R7: 100%|██████████| 5007/5007 [00:04<00:00, 1236.80file/s]\n",
      "Processing files in R7: 100%|██████████| 4893/4893 [00:02<00:00, 1635.15file/s]\n",
      "Processing files in R7: 100%|██████████| 6011/6011 [00:03<00:00, 1756.38file/s]\n",
      "Processing files in R7: 100%|██████████| 4602/4602 [00:02<00:00, 1551.99file/s]\n",
      "Processing files in R7: 100%|██████████| 3037/3037 [00:01<00:00, 1927.85file/s]\n",
      "Processing files in R7: 0file [00:00, ?file/s]\n",
      "Processing files in R7: 100%|██████████| 7110/7110 [00:04<00:00, 1500.89file/s]\n",
      "Processing files in R7: 100%|██████████| 4900/4900 [00:02<00:00, 1751.40file/s]\n",
      "Processing files in R7: 100%|██████████| 7332/7332 [00:04<00:00, 1700.47file/s]\n",
      "Processing files in R7: 100%|██████████| 5102/5102 [00:02<00:00, 1843.54file/s]\n",
      "Processing files in R7: 100%|██████████| 7372/7372 [00:03<00:00, 1914.99file/s]\n",
      "Processing files in R7: 100%|██████████| 5128/5128 [00:03<00:00, 1682.95file/s]\n",
      "Processing files in R7: 100%|██████████| 5836/5836 [00:03<00:00, 1528.23file/s]\n",
      "Processing files in R7: 100%|██████████| 5810/5810 [00:03<00:00, 1640.31file/s]\n",
      "Processing files in R7: 100%|██████████| 7088/7088 [00:04<00:00, 1698.70file/s]\n",
      "Processing files in R7: 100%|██████████| 7704/7704 [00:04<00:00, 1553.80file/s]\n",
      "Processing files in R7: 100%|██████████| 7288/7288 [00:05<00:00, 1389.72file/s]\n",
      "Processing files in R7: 100%|██████████| 4692/4692 [00:03<00:00, 1483.23file/s]\n",
      "Processing files in R8: 0file [00:00, ?file/s]\n",
      "Processing files in R8: 0file [00:00, ?file/s]\n",
      "Processing files in R8: 100%|██████████| 3493/3493 [00:01<00:00, 2336.22file/s]\n",
      "Processing files in R8: 100%|██████████| 2755/2755 [00:01<00:00, 1680.22file/s]\n",
      "Processing files in R8: 100%|██████████| 3305/3305 [00:01<00:00, 2403.16file/s]\n",
      "Processing files in R8: 100%|██████████| 3578/3578 [00:02<00:00, 1601.72file/s]\n",
      "Processing files in R8: 100%|██████████| 2763/2763 [00:02<00:00, 1349.63file/s]\n",
      "Processing files in R8: 100%|██████████| 3997/3997 [00:02<00:00, 1356.85file/s]\n",
      "Processing files in R8: 100%|██████████| 2942/2942 [00:02<00:00, 1357.91file/s]\n",
      "Processing files in R8: 100%|██████████| 3352/3352 [00:02<00:00, 1374.46file/s]\n",
      "Processing files in R8: 100%|██████████| 3205/3205 [00:02<00:00, 1454.25file/s]\n",
      "Processing files in R8: 100%|██████████| 3220/3220 [00:01<00:00, 1725.64file/s]\n",
      "Processing files in R8: 100%|██████████| 3755/3755 [00:01<00:00, 1974.32file/s]\n",
      "Processing files in R8: 100%|██████████| 3821/3821 [00:02<00:00, 1419.48file/s]\n",
      "Processing files in R8: 0file [00:00, ?file/s]\n",
      "Processing files in R8: 100%|██████████| 4864/4864 [00:02<00:00, 1789.51file/s]\n",
      "Processing files in R8: 100%|██████████| 4058/4058 [00:02<00:00, 1381.88file/s]\n",
      "Processing files in R8: 100%|██████████| 4824/4824 [00:02<00:00, 2339.48file/s]\n",
      "Processing files in R8: 100%|██████████| 4841/4841 [00:02<00:00, 1655.42file/s]\n",
      "Processing files in R8: 100%|██████████| 4079/4079 [00:02<00:00, 1449.92file/s]\n",
      "Processing files in R8: 100%|██████████| 5151/5151 [00:03<00:00, 1518.01file/s]\n",
      "Processing files in R8: 100%|██████████| 4659/4659 [00:03<00:00, 1366.18file/s]\n",
      "Processing files in R8: 100%|██████████| 5055/5055 [00:02<00:00, 2010.24file/s]\n",
      "Processing files in R8: 100%|██████████| 4845/4845 [00:03<00:00, 1552.17file/s]\n",
      "Processing files in R8: 100%|██████████| 4664/4664 [00:03<00:00, 1436.93file/s]\n",
      "Processing files in R8: 100%|██████████| 4981/4981 [00:02<00:00, 1706.17file/s]\n",
      "Processing files in R8: 100%|██████████| 5104/5104 [00:02<00:00, 1774.80file/s]\n",
      "Processing files in R9: 0file [00:00, ?file/s]\n",
      "Processing files in R9: 0file [00:00, ?file/s]\n",
      "Processing files in R9: 100%|██████████| 3972/3972 [00:02<00:00, 1565.28file/s]\n",
      "Processing files in R9: 100%|██████████| 3209/3209 [00:02<00:00, 1419.97file/s]\n",
      "Processing files in R9: 100%|██████████| 3859/3859 [00:02<00:00, 1582.52file/s]\n",
      "Processing files in R9: 100%|██████████| 2916/2916 [00:02<00:00, 1399.08file/s]\n",
      "Processing files in R9: 100%|██████████| 3501/3501 [00:02<00:00, 1733.11file/s]\n",
      "Processing files in R9: 100%|██████████| 3951/3951 [00:02<00:00, 1630.97file/s]\n",
      "Processing files in R9: 100%|██████████| 4140/4140 [00:02<00:00, 1586.28file/s]\n",
      "Processing files in R9: 100%|██████████| 3511/3511 [00:01<00:00, 1797.71file/s]\n",
      "Processing files in R9: 100%|██████████| 3378/3378 [00:02<00:00, 1454.57file/s]\n",
      "Processing files in R9: 100%|██████████| 3307/3307 [00:01<00:00, 1730.23file/s]\n",
      "Processing files in R9: 100%|██████████| 3574/3574 [00:01<00:00, 1881.92file/s]\n",
      "Processing files in R9: 100%|██████████| 3203/3203 [00:01<00:00, 1766.28file/s]\n",
      "Processing files in R9: 0file [00:00, ?file/s]\n",
      "Processing files in R9: 100%|██████████| 5332/5332 [00:03<00:00, 1563.65file/s]\n",
      "Processing files in R9: 100%|██████████| 4739/4739 [00:02<00:00, 1657.34file/s]\n",
      "Processing files in R9: 100%|██████████| 5074/5074 [00:02<00:00, 2096.61file/s]\n",
      "Processing files in R9: 100%|██████████| 4360/4360 [00:02<00:00, 1555.97file/s]\n",
      "Processing files in R9: 100%|██████████| 4741/4741 [00:02<00:00, 1597.83file/s]\n",
      "Processing files in R9: 100%|██████████| 4645/4645 [00:02<00:00, 1899.04file/s]\n",
      "Processing files in R9: 100%|██████████| 5517/5517 [00:03<00:00, 1542.36file/s]\n",
      "Processing files in R9: 100%|██████████| 4751/4751 [00:02<00:00, 1832.09file/s]\n",
      "Processing files in R9: 100%|██████████| 4282/4282 [00:03<00:00, 1271.47file/s]\n",
      "Processing files in R9: 100%|██████████| 4663/4663 [00:03<00:00, 1480.81file/s]\n",
      "Processing files in R9: 100%|██████████| 4109/4109 [00:02<00:00, 1695.27file/s]\n",
      "Processing files in R9: 100%|██████████| 4176/4176 [00:02<00:00, 1643.74file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matched files: 326654\n",
      "Matched file information saved to 'matched_files_labels.csv'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to extract coordinates from file name\n",
    "def extract_coordinates(file_path):\n",
    "    file_name = os.path.basename(file_path)\n",
    "    # Adjust the regex pattern if your file naming convention changes\n",
    "    pattern = r\"[a-z]*_image_([+-]?\\d+\\.\\d+)_([+-]?\\d+\\.\\d+)_\\d{4}-\\d{2}-\\d{2}\\.tif\"\n",
    "    match = re.match(pattern, file_name)\n",
    "    if match:\n",
    "        latitude = float(match.group(1))\n",
    "        longitude = float(match.group(2))\n",
    "        return latitude, longitude\n",
    "    else:\n",
    "        raise ValueError(f\"Filename '{file_name}' does not match the expected pattern.\")\n",
    "\n",
    "# Function to match coordinates in the DataFrame\n",
    "def match_coordinates(df, latitude, longitude, tolerance=1e-5):\n",
    "    matched_rows = df[\n",
    "        (df['EA_GPS_LA'].between(latitude - tolerance, latitude + tolerance)) &\n",
    "        (df['EA_GPS_LO'].between(longitude - tolerance, longitude + tolerance))\n",
    "    ]\n",
    "    return matched_rows\n",
    "\n",
    "# Function to ensure directory exists\n",
    "def ensure_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "# Paths configuration\n",
    "dataset_directory = \"/work/lamlab/data\"\n",
    "\n",
    "csv_file_path = \"/work/lamlab/data/MERGED.csv\"\n",
    "\n",
    "# Output directories\n",
    "structured_dataset_b = \"/work/lamlab/data/PIPEDWATER\"\n",
    "structured_dataset_c = \"/work/lamlab/data/SEWAGE\"\n",
    "\n",
    "# Split ratios\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.1\n",
    "test_ratio = 0.1\n",
    "\n",
    "# Load CSV\n",
    "try:\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    print(\"CSV file loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading CSV file: {e}\")\n",
    "\n",
    "# Check if required columns exist\n",
    "required_columns = {'EA_GPS_LA', 'EA_GPS_LO', 'EA_SVC_B', 'EA_SVC_C'}\n",
    "if not required_columns.issubset(df.columns):\n",
    "    print(f\"CSV does not contain required columns: {required_columns}\")\n",
    "\n",
    "# Initialize a list to store matched file information\n",
    "matched_files = []\n",
    "\n",
    "# Iterate over all .tif files in the dataset directory\n",
    "print(\"Scanning .tif files and matching coordinates...\")\n",
    "subfolders = ['R7', 'R8', 'R9']\n",
    "for sub in subfolders:\n",
    "    subdir_path = os.path.join(dataset_directory, sub)\n",
    "    if not os.path.exists(subdir_path):\n",
    "        print(f\"Subfolder '{sub}' not found at {subdir_path}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    for root, dirs, files in os.walk(subdir_path):\n",
    "        for file in tqdm(files, desc=f\"Processing files in {sub}\", unit=\"file\"):\n",
    "            if file.lower().endswith(\".tif\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    lat, lon = extract_coordinates(file_path)\n",
    "                except ValueError as ve:\n",
    "                    print(f\"Skipping file '{file_path}': {ve}\")\n",
    "                    continue\n",
    "\n",
    "                matched = match_coordinates(df, lat, lon)\n",
    "\n",
    "                if not matched.empty:\n",
    "                    row = matched.iloc[0]\n",
    "                    ea_svc_b = row['EA_SVC_B']\n",
    "                    ea_svc_c = row['EA_SVC_C']\n",
    "                    matched_files.append({\n",
    "                        'file_path': file_path,\n",
    "                        'EA_SVC_B': ea_svc_b,\n",
    "                        'EA_SVC_C': ea_svc_c\n",
    "                    })\n",
    "                else:\n",
    "                    print(f\"No match found for file '{file_path}'.\")\n",
    "\n",
    "# Convert matched_files to a DataFrame\n",
    "matched_df = pd.DataFrame(matched_files)\n",
    "print(f\"Total matched files: {len(matched_df)}\")\n",
    "\n",
    "# Check if there are matched files\n",
    "if matched_df.empty:\n",
    "    print(\"No matched files found. Exiting.\")\n",
    "\n",
    "# Save matched information to a CSV (optional)\n",
    "matched_df = matched_df[((matched_df['EA_SVC_B']==0) | (matched_df['EA_SVC_B']==1)) & ((matched_df['EA_SVC_C']==0) | (matched_df['EA_SVC_C']==1))]\n",
    "matched_df.to_csv(\"/work/lamlab/matched_files_labels.csv\", index=False)\n",
    "print(\"Matched file information saved to 'matched_files_labels.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data into train, validation, and test sets...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 259918 files\n",
      "Validation set: 32490 files\n",
      "Test set: 32490 files\n",
      "Copying files to structured directories...\n",
      "Copying train set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train set:  28%|██▊       | 73359/259918 [06:57<18:36, 167.04it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Splitting the data\n",
    "print(\"Splitting data into train, validation, and test sets...\")\n",
    "\n",
    "# First split into train and temp (train: 80%, temp: 20%)\n",
    "train_df, temp_df = train_test_split(\n",
    "    matched_df,\n",
    "    test_size=(val_ratio + test_ratio),\n",
    "    stratify=matched_df['EA_SVC_C'],  # Stratify based on EA_SVC_C to maintain label distribution\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Then split temp into validation and test (each 10%)\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=test_ratio / (val_ratio + test_ratio),\n",
    "    stratify=temp_df['EA_SVC_C'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(train_df)} files\")\n",
    "print(f\"Validation set: {len(val_df)} files\")\n",
    "print(f\"Test set: {len(test_df)} files\")\n",
    "\n",
    "# Ensure the directory exists\n",
    "def ensure_dir(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "# Function to copy a single file\n",
    "def copy_file(row, split_name):\n",
    "    # For EA_SVC_C\n",
    "    label_b = row['EA_SVC_C']\n",
    "    dest_dir_b = os.path.join(structured_dataset_c, split_name, str(label_b))\n",
    "    ensure_dir(dest_dir_b)\n",
    "    shutil.copy2(row['file_path'], dest_dir_b)\n",
    "\n",
    "# Function to copy files for a split using ThreadPoolExecutor\n",
    "def copy_files_parallel(df_split, split_name):\n",
    "    with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "        # Submit tasks for parallel execution\n",
    "        futures = [executor.submit(copy_file, row, split_name) for _, row in df_split.iterrows()]\n",
    "        # Track progress\n",
    "        for _ in tqdm(as_completed(futures), desc=f\"Processing {split_name} set\", total=len(df_split)):\n",
    "            pass\n",
    "\n",
    "# Copy files for each split\n",
    "print(\"Copying files to structured directories...\")\n",
    "for split_name, df_split in zip(['train', 'val', 'test'], [train_df, val_df, test_df]):\n",
    "    print(f\"Copying {split_name} set...\")\n",
    "    copy_files_parallel(df_split, split_name)\n",
    "\n",
    "print(\"All files have been copied to their respective directories.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data into train, validation, and test sets...\n",
      "Training set: 259918 files\n",
      "Validation set: 32490 files\n",
      "Test set: 32490 files\n",
      "Copying files to structured directories...\n",
      "Copying train set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train set: 100%|██████████| 259918/259918 [19:58<00:00, 216.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying val set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing val set: 100%|██████████| 32490/32490 [02:22<00:00, 227.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test set: 100%|██████████| 32490/32490 [02:22<00:00, 227.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files have been copied to their respective directories.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Splitting the data\n",
    "print(\"Splitting data into train, validation, and test sets...\")\n",
    "\n",
    "# First split into train and temp (train: 80%, temp: 20%)\n",
    "train_df, temp_df = train_test_split(\n",
    "    matched_df,\n",
    "    test_size=(val_ratio + test_ratio),\n",
    "    stratify=matched_df['EA_SVC_B'],  # Stratify based on EA_SVC_B to maintain label distribution\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Then split temp into validation and test (each 10%)\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=test_ratio / (val_ratio + test_ratio),\n",
    "    stratify=temp_df['EA_SVC_B'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(train_df)} files\")\n",
    "print(f\"Validation set: {len(val_df)} files\")\n",
    "print(f\"Test set: {len(test_df)} files\")\n",
    "\n",
    "# Ensure the directory exists\n",
    "def ensure_dir(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "# Function to copy a single file\n",
    "def copy_file(row, split_name):\n",
    "    # For EA_SVC_B\n",
    "    label_b = row['EA_SVC_B']\n",
    "    dest_dir_b = os.path.join(structured_dataset_b, split_name, str(label_b))\n",
    "    ensure_dir(dest_dir_b)\n",
    "    shutil.copy2(row['file_path'], dest_dir_b)\n",
    "\n",
    "# Function to copy files for a split using ThreadPoolExecutor\n",
    "def copy_files_parallel(df_split, split_name):\n",
    "    with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "        # Submit tasks for parallel execution\n",
    "        futures = [executor.submit(copy_file, row, split_name) for _, row in df_split.iterrows()]\n",
    "        # Track progress\n",
    "        for _ in tqdm(as_completed(futures), desc=f\"Processing {split_name} set\", total=len(df_split)):\n",
    "            pass\n",
    "\n",
    "# Copy files for each split\n",
    "print(\"Copying files to structured directories...\")\n",
    "for split_name, df_split in zip(['train', 'val', 'test'], [train_df, val_df, test_df]):\n",
    "    print(f\"Copying {split_name} set...\")\n",
    "    copy_files_parallel(df_split, split_name)\n",
    "\n",
    "print(\"All files have been copied to their respective directories.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separating Landsat and Sentinel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "\n",
    "def detect_sensor(filename):\n",
    "    fn = filename.lower()\n",
    "    if 'sentinel' in fn:\n",
    "        return 'Sentinel'\n",
    "    if 'landsat' in fn:\n",
    "        return 'Landsat'\n",
    "    return None\n",
    "\n",
    "def find_split_and_class(root):\n",
    "    \"\"\"\n",
    "    Finds the split (train/val/test) and class label (e.g. 0/1) from a path.\n",
    "    Returns (split, class_label) or (None, None) if not found.\n",
    "    \"\"\"\n",
    "    parts = [p for p in root.split(os.sep) if p]\n",
    "    for s in ('train', 'val', 'test'):\n",
    "        if s in parts:\n",
    "            idx = parts.index(s)\n",
    "            class_label = parts[idx+1] if idx+1 < len(parts) else ''\n",
    "            return s, class_label\n",
    "    return None, None\n",
    "\n",
    "def create_dest_path(root, file, new_base):\n",
    "    \"\"\"\n",
    "    Generate destination path preserving split/class structure and separating sensors.\n",
    "    Example:\n",
    "      /work/lamlab/PIPEDWATER/train/0/sentinel_image_...tif\n",
    "      -> /work/lamlab/PIPEDWATER_by_sensor/Sentinel/train/0/sentinel_image_...tif\n",
    "    \"\"\"\n",
    "    sensor = detect_sensor(file)\n",
    "    if sensor is None:\n",
    "        return None\n",
    "    split, class_label = find_split_and_class(root)\n",
    "    if split is None:\n",
    "        return None\n",
    "    return os.path.join(new_base, sensor, split, class_label, file)\n",
    "\n",
    "def process_file_pair(args):\n",
    "    src_path, dest_path = args\n",
    "    os.makedirs(os.path.dirname(dest_path), exist_ok=True)\n",
    "    shutil.copy2(src_path, dest_path)\n",
    "    return True\n",
    "\n",
    "def process_dataset(dataset):\n",
    "    \"\"\"\n",
    "    Process all .tif files under /work/lamlab/{dataset}/{train,val,test}/... and copy them to\n",
    "    /work/lamlab/{dataset}_by_sensor/{Sentinel|Landsat}/{train,val,test}/{class}/...\n",
    "    \"\"\"\n",
    "    original_base = f\"/work/lamlab/{dataset}\"\n",
    "    new_base = f\"/work/lamlab/{dataset}_by_sensor\"\n",
    "\n",
    "    if not os.path.isdir(original_base):\n",
    "        print(f\"Source folder not found: {original_base}, skipping.\")\n",
    "        return\n",
    "\n",
    "    # Collect file pairs\n",
    "    file_pairs = []\n",
    "    for root, _, files in os.walk(original_base):\n",
    "        for file in files:\n",
    "            if not file.lower().endswith(\".tif\"):\n",
    "                continue\n",
    "            dest_path = create_dest_path(root, file, new_base)\n",
    "            if dest_path:\n",
    "                file_pairs.append((os.path.join(root, file), dest_path))\n",
    "            else:\n",
    "                # optional: uncomment next line to see skipped files\n",
    "                # print(f\"Skipping {os.path.join(root, file)} (no sensor or split/class detected)\")\n",
    "                pass\n",
    "\n",
    "    if not file_pairs:\n",
    "        print(f\"No .tif files found for {dataset} (or none matched sensor+split structure).\")\n",
    "        return\n",
    "\n",
    "    # Use all available CPUs; copying is IO-bound so a moderate number is fine\n",
    "    max_workers = min(os.cpu_count() or 1, 8)\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(process_file_pair, pair) for pair in file_pairs]\n",
    "        with tqdm(total=len(file_pairs), desc=f\"Processing {dataset} files\", unit=\"file\", dynamic_ncols=True) as pbar:\n",
    "            for future in as_completed(futures):\n",
    "                try:\n",
    "                    future.result()\n",
    "                except Exception as e:\n",
    "                    tqdm.write(f\"Error copying file: {e}\")\n",
    "                pbar.update(1)\n",
    "\n",
    "def main():\n",
    "    for dataset in ['PIPEDWATER', 'SEWAGE']:\n",
    "        process_dataset(dataset)\n",
    "    print(\"Dataset reorganization completed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47375\n",
      "59063\n",
      "33572\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(len(os.listdir('/work/lamlab/PIPEDWATER_by_sensor/Sentinel/train/1.0')))\n",
    "print(len(os.listdir('/work/lamlab/SEWAGE_by_sensor/Sentinel/train/1.0')))\n",
    "print(len(os.listdir('/work/lamlab/PIPEDWATER_by_sensor/Landsat/train/1.0')))\n",
    "print(len(os.listdir('/work/lamlab/SEWAGE_by_sensor/Landsat/train/1.0')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URBAN VS RURAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18631 files in the val/ split to process.\n",
      "Using 94 CPUs for processing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 18631/18631 [01:51<00:00, 167.08it/s]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Parameters: update these paths as needed ---\n",
    "BASE_DIR = \"/work/lamlab/TEST_C/Sentinel\"\n",
    "CSV_PATH = \"/dkucc/home/oe23/DATABASE/RAW/MERGED.csv\"\n",
    "# Only process the 'val' split\n",
    "SPLITS = [\"val\"]\n",
    "\n",
    "# --- Functions ---\n",
    "\n",
    "def extract_coordinates(file_path):\n",
    "    \"\"\"\n",
    "    Extracts coordinates from a file name matching pattern:\n",
    "    <anything>_image_<latitude>_<longitude>_YYYY-MM-DD.tif\n",
    "    \"\"\"\n",
    "    file_name = os.path.basename(file_path)\n",
    "    pattern = r\"[a-zA-Z0-9_]+_image_([+-]?\\d+\\.\\d+)_([+-]?\\d+\\.\\d+)_\\d{4}-\\d{2}-\\d{2}\\.tif\"\n",
    "    match = re.match(pattern, file_name)\n",
    "    if match:\n",
    "        return float(match.group(1)), float(match.group(2))\n",
    "    raise ValueError(f\"Filename '{file_name}' doesn't match expected pattern.\")\n",
    "\n",
    "def match_coordinates(df, latitude, longitude, tolerance=1e-5):\n",
    "    \"\"\"\n",
    "    Finds the row in the DataFrame where EA_GPS_LA and EA_GPS_LO are within a tolerance.\n",
    "    \"\"\"\n",
    "    matched = df[\n",
    "        (df['EA_GPS_LA'].between(latitude - tolerance, latitude + tolerance)) &\n",
    "        (df['EA_GPS_LO'].between(longitude - tolerance, longitude + tolerance))\n",
    "    ]\n",
    "    if matched.empty:\n",
    "        raise ValueError(f\"No match found in CSV for coordinates ({latitude}, {longitude}).\")\n",
    "    return matched.iloc[0]  # Return the first match\n",
    "\n",
    "def process_file(file_info, df, base_dir):\n",
    "    \"\"\"\n",
    "    Processes one file:\n",
    "      - file_info: (split, class_label, full_path)\n",
    "      - Extracts coordinates from filename.\n",
    "      - Matches them in df to get URBRUR value.\n",
    "      - Determines the destination folder based on the 'val' split and the URBRUR value.\n",
    "      - Copies the file to the new destination while preserving the class (0 or 1) subfolder.\n",
    "    \"\"\"\n",
    "    split, class_label, file_path = file_info\n",
    "    try:\n",
    "        # Extract coordinates from filename\n",
    "        lat, lon = extract_coordinates(file_path)\n",
    "        # Match coordinates in DataFrame to retrieve the row and its URBRUR value\n",
    "        row = match_coordinates(df, lat, lon)\n",
    "        urbrur_value = row['URBRUR']\n",
    "        \n",
    "        # Create a destination folder name based on the URBRUR value.\n",
    "        # For instance, if URBRUR is 1.0, destination becomes \"val_1.0\"\n",
    "        # If you prefer a different naming convention (e.g., removing the decimal),\n",
    "        # you can adjust the following line.\n",
    "        dest_split = f\"{split}_{urbrur_value}\"\n",
    "        \n",
    "        # Build the full destination directory path, preserving the class (0 or 1)\n",
    "        dest_dir = os.path.join(base_dir, dest_split, class_label)\n",
    "        os.makedirs(dest_dir, exist_ok=True)\n",
    "\n",
    "        # Copy the file to the destination folder\n",
    "        dest_path = os.path.join(dest_dir, os.path.basename(file_path))\n",
    "        shutil.copy2(file_path, dest_path)\n",
    "    except Exception as e:\n",
    "        # Log errors; you could also write to a log file if preferred\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "\n",
    "def gather_files(base_dir, splits):\n",
    "    \"\"\"\n",
    "    Gathers all .tif file paths from the specified splits.\n",
    "    Returns a list of tuples: (split, class_label, file_path).\n",
    "    \"\"\"\n",
    "    file_list = []\n",
    "    for split in splits:\n",
    "        split_dir = os.path.join(base_dir, split)\n",
    "        if not os.path.isdir(split_dir):\n",
    "            print(f\"Warning: {split_dir} does not exist, skipping...\")\n",
    "            continue\n",
    "        for class_label in [\"0\", \"1\"]:\n",
    "            class_dir = os.path.join(split_dir, class_label)\n",
    "            if not os.path.isdir(class_dir):\n",
    "                print(f\"Warning: {class_dir} does not exist, skipping...\")\n",
    "                continue\n",
    "            for fname in os.listdir(class_dir):\n",
    "                if fname.lower().endswith(\".tif\"):\n",
    "                    file_path = os.path.join(class_dir, fname)\n",
    "                    file_list.append((split, class_label, file_path))\n",
    "    return file_list\n",
    "\n",
    "def main():\n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    # Optionally, you can inspect the DataFrame's head:\n",
    "    # print(df.head())\n",
    "\n",
    "    # Gather all .tif files from the 'val' split\n",
    "    files_to_process = gather_files(BASE_DIR, SPLITS)\n",
    "    print(f\"Found {len(files_to_process)} files in the val/ split to process.\")\n",
    "\n",
    "    # Use all available CPUs\n",
    "    cpus = multiprocessing.cpu_count()\n",
    "    print(f\"Using {cpus} CPUs for processing.\")\n",
    "\n",
    "    # Prepare the worker function with additional parameters using partial\n",
    "    worker = partial(process_file, df=df, base_dir=BASE_DIR)\n",
    "    \n",
    "    # Use multiprocessing Pool with tqdm progress bar\n",
    "    with multiprocessing.Pool(processes=cpus) as pool:\n",
    "        # tqdm will display progress as files are processed.\n",
    "        for _ in tqdm(pool.imap_unordered(worker, files_to_process), total=len(files_to_process)):\n",
    "            pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dino",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
